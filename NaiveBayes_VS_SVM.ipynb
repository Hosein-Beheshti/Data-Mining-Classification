{
 "cells": [
  {
   "source": [
    "# Classification Using NB and SVM\n",
    "in this project we are going to classify IMDB review comments with naive bayes and SVM classifiers using sklearn library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c71207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset by path and file's name\n",
    "def read_dataset(path, name):\n",
    "    myFile = open(path + name, 'r')\n",
    "    return myFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myFile = read_dataset(\"datasets/\", \"IMDB_review_labels.txt\")\n",
    "lines = myFile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    #Normalization-------------\n",
    "    #delete punctions\n",
    "    edited_doc = doc.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "\n",
    "    #lower case\n",
    "    edited_doc = edited_doc.lower()\n",
    "\n",
    "    #tokenize-----------------\n",
    "    tokenized_doc = edited_doc.split()\n",
    "    ps = PorterStemmer()\n",
    "    tokens = []\n",
    "    for token in tokenized_doc:\n",
    "        #stemming\n",
    "        tokens.append(ps.stem(token))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = np.zeros(len(lines))\n",
    "tokensPerDoc = []\n",
    "len_docs = len(lines)\n",
    "for line in lines:\n",
    "    doc_id = lines.index(line)\n",
    "    tokens = tokenizer(line)\n",
    "    labels[doc_id] = tokens[-1]\n",
    "    tokens.pop(-1)\n",
    "    tokensPerDoc.append(tokens)\n",
    "\n",
    "tokensPerDoc[0]\n",
    "mergedTokens = []\n",
    "for i in range(len(tokensPerDoc)):\n",
    "    mergedTokens = mergedTokens + tokensPerDoc[i]\n",
    "\n",
    "terms = np.unique(mergedTokens)\n",
    "\n",
    "inverted_index = np.zeros((len_docs, len(terms)))\n",
    "\n",
    "terms_dict = dict()\n",
    "for i in range(len(terms)):\n",
    "    terms_dict[terms[i]] = i\n",
    "\n",
    "#calculate weights using tfidf\n",
    "#tf\n",
    "for i in range(len(tokensPerDoc)):\n",
    "    (unique, counts) = np.unique(tokensPerDoc[i], return_counts=True)\n",
    "    temrsWithFrequency = np.asarray((unique, counts)).T\n",
    "\n",
    "    for term in temrsWithFrequency:\n",
    "        if term[0] in terms_dict:\n",
    "            index = terms_dict[term[0]]\n",
    "            tf = math.log(1 + int(term[1]), 10)\n",
    "            inverted_index[i][index] = tf\n",
    "\n",
    "#idf\n",
    "for i in range(len(terms)):\n",
    "    idf = math.log(len_docs / np.count_nonzero(inverted_index[: ,i]))\n",
    "    inverted_index[:, i] = inverted_index[:, i] * idf\n"
   ]
  },
  {
   "source": [
    "### classifying Using Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy:  67.5\nerror:  32.49999999999999\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(inverted_index, labels, test_size=0.2, random_state=0)\n",
    "gNB = GaussianNB()\n",
    "y_pred = gNB.fit(X_train, y_train).predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "error = 1 - accuracy\n",
    "print(\"accuracy: \", accuracy*100)\n",
    "print(\"error: \", error*100)\n",
    "\n"
   ]
  },
  {
   "source": [
    "### classifying Using SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy:  70.5\nerror:  29.500000000000004\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(inverted_index, labels, test_size=0.2, random_state=0)\n",
    "mySvm = svm.SVC()\n",
    "y_pred = mySvm.fit(X_train, y_train).predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "error = 1 - accuracy\n",
    "print(\"accuracy: \", accuracy*100)\n",
    "print(\"error: \", error*100)\n",
    "\n"
   ]
  },
  {
   "source": [
    "#### comparison\n",
    "As we can see svm predicted with a higher accuracy. Both SVM and NB are good classifying methods, but NB is very sensitive in feature selection, so that can affect on accuracy. Moreover, NB is so simple and it consider attributes independence so it may decrease it's accuracy. However, that's the reason that make it so fast and easy to use. On the otherside, Svm is one of the best classifying methods for both linear and non-linear datasets, but it has a long and heavy training proccess."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "7ec3019a7cb8f5739f8b27d55fc304c51580b268fd2b34191f81e31e11772118"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}